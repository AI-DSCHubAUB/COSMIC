{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install bert_score\n",
        "!pip install evaluate\n",
        "!pip install rouge_score\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "6WOB_oh7_SDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"stanfordnlp/snli\") #we used load_dataset(\"snli\") but it seems it got changed to this\n",
        "# Or to use the mnli dataset\n",
        "# dataset = load_dataset(\"nyu-mll/multi_nli\") #we used load_dataset(\"multi_nli\") but it seems it got changed to this"
      ],
      "metadata": {
        "id": "RlS8ptQp_PuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "premise = dataset['test']['premise'][:]\n",
        "hypothesis = dataset['test']['hypothesis'][:]"
      ],
      "metadata": {
        "id": "sw59vkSWAFWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s1 = dataset['train']['premise'][:]\n",
        "s2 = dataset['train']['hypothesis'][:]"
      ],
      "metadata": {
        "id": "dODLQTngAcTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_score import BERTScorer\n",
        "scorer = BERTScorer(lang=\"en\",rescale_with_baseline=True)"
      ],
      "metadata": {
        "id": "i-eaUB1n-jSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUqb-MnK8H1j"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import pickle\n",
        "def bulk_bertscore(premise_array,hypothesis_array,step=10000, save_name = 'all_bertscore_scores_snli_.pkl'):\n",
        "\n",
        "    scores = {\n",
        "        'P':[],\n",
        "        'R':[],\n",
        "        'F1':[],\n",
        "    }\n",
        "    for i in tqdm(range(0,len(premise_array),step)):\n",
        "        premise = premise_array[i:i+step]\n",
        "        hypothesis = hypothesis_array[i:i+step]\n",
        "\n",
        "        P,R,F1 = scorer.score(premise,hypothesis)\n",
        "\n",
        "        scores['P'] = scores['P'] + P.tolist()\n",
        "        scores['R'] = scores['R'] + R.tolist()\n",
        "        scores['F1'] = scores['F1'] + F1.tolist()\n",
        "\n",
        "        del P\n",
        "        del R\n",
        "        del F1\n",
        "\n",
        "        with open(save_name, 'wb') as f:\n",
        "            pickle.dump(scores, f)\n",
        "\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_scores = bulk_bertscore(s1,s2, step=10000, save_name = 'all_bertscore_scores_snli_.pkl')"
      ],
      "metadata": {
        "id": "8vIxqPdoAVeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "nli_classifier = pipeline(\"text-classification\", model = \"microsoft/deberta-large-mnli\", device=0)"
      ],
      "metadata": {
        "id": "pnIOD65KA56V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "similarity_model = SentenceTransformer('all-mpnet-base-v2')"
      ],
      "metadata": {
        "id": "AuNBDcPIA_2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def cosmic(arr1,arr2):\n",
        "    similarity_model.max_seq_length = 512\n",
        "    arr_combined = arr1+arr2\n",
        "    embeddings = similarity_model.encode(arr_combined)\n",
        "    similarity_scores = []\n",
        "    for i in (range(len(embeddings)//2)):\n",
        "        arr = np.array([np.array(embeddings[i]),np.array(embeddings[i+len(embeddings)//2])])\n",
        "        similarity_scores.append(max(cosine_similarity(arr)[0][1],0))\n",
        "#         '[CLS]'+premise[i]+'[SEP]'+hypothesis[i]+'[SEP]\n",
        "    arr2 = [arr_combined[i]+' '+arr_combined[i+len(arr_combined)//2] for i in tqdm(range(len(arr_combined)//2))]\n",
        "    classifier_results = entailment_classifier2(arr2,return_all_scores=True,truncation=True, max_length=512, batch_size=100)\n",
        "    entailment_scores = []\n",
        "    for i in (range(len(classifier_results))):\n",
        "        entailment_scores.append(1-classifier_results[i][0]['score'])\n",
        "    cosmic_scores = []\n",
        "    for i in (range(len(classifier_results))):\n",
        "        cosmic_scores.append(similarity_scores[i]*entailment_scores[i])\n",
        "    return cosmic_scores,similarity_scores,entailment_scores"
      ],
      "metadata": {
        "id": "9J8aa7ixBD0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import pickle\n",
        "def bulk_cosmic(premise_array,hypothesis_array,step=10000, save_name = 'all_scores_snli_.pkl'):\n",
        "\n",
        "    scores = {\n",
        "        'cosmic':[],\n",
        "        'similarity':[],\n",
        "        'consistency':[],\n",
        "    }\n",
        "    for i in tqdm(range(0,len(premise_array),step)):\n",
        "        premise = premise_array[i:i+step]\n",
        "        hypothesis = hypothesis_array[i:i+step]\n",
        "\n",
        "        intermediate_cosmic,intermediate_similarity,intermediate_consistency = cosmic(premise,hypothesis)\n",
        "\n",
        "        scores['cosmic'] = scores['cosmic'] + intermediate_cosmic\n",
        "        scores['similarity'] = scores['similarity'] + intermediate_similarity\n",
        "        scores['consistency'] = scores['consistency'] + intermediate_consistency\n",
        "\n",
        "        del intermediate_cosmic\n",
        "        del intermediate_similarity\n",
        "        del intermediate_consistency\n",
        "\n",
        "        with open(save_name, 'wb') as f:\n",
        "            pickle.dump(scores, f)\n",
        "\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "aLEq9OhfAkVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosmic_scores_snli = bulk_cosmic(s1, s2, save_name = 'all_scores_snli_2024.pkl')"
      ],
      "metadata": {
        "id": "sOvN5fLJAqQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_score(arr1,arr2):\n",
        "    similarity_model.max_seq_length = 512\n",
        "    arr_combined = arr1+arr2\n",
        "    embeddings = similarity_model.encode(arr_combined)**5\n",
        "    similarity_scores = []\n",
        "    for i in (range(len(embeddings)//2)):\n",
        "        arr = np.array([np.array(embeddings[i]),np.array(embeddings[i+len(embeddings)//2])])\n",
        "        similarity_scores.append(max(cosine_similarity(arr)[0][1],0))\n",
        "    return similarity_scores"
      ],
      "metadata": {
        "id": "2x2gWvhUBbRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import pickle\n",
        "def bulk_similarity(premise_array,hypothesis_array,step=10000, save_name = 'all_similarity_scores_snli_2024.pkl'):\n",
        "\n",
        "    scores = {\n",
        "        'score':[]\n",
        "    }\n",
        "    for i in tqdm(range(0,len(premise_array),step)):\n",
        "        premise = premise_array[i:i+step]\n",
        "        hypothesis = hypothesis_array[i:i+step]\n",
        "\n",
        "        similarities = similarity_score(premise,hypothesis)\n",
        "\n",
        "        scores['score'] = scores['score'] + similarities\n",
        "\n",
        "        del similarities\n",
        "\n",
        "        with open(save_name, 'wb') as f:\n",
        "            pickle.dump(scores, f)\n",
        "\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "JwqFhp-BBgKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bulk_similarity_scores = bulk_similarity(s1, s2)"
      ],
      "metadata": {
        "id": "xw6nmC3sBtZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BLEU Score code from the evaluate library https://github.com/huggingface/evaluate/blob/main/metrics/bleu/bleu.py\n",
        "\n",
        "import collections\n",
        "import math\n",
        "\n",
        "\n",
        "def _get_ngrams(segment, max_order):\n",
        "  \"\"\"Extracts all n-grams upto a given maximum order from an input segment.\n",
        "\n",
        "  Args:\n",
        "    segment: text segment from which n-grams will be extracted.\n",
        "    max_order: maximum length in tokens of the n-grams returned by this\n",
        "        methods.\n",
        "\n",
        "  Returns:\n",
        "    The Counter containing all n-grams upto max_order in segment\n",
        "    with a count of how many times each n-gram occurred.\n",
        "  \"\"\"\n",
        "  ngram_counts = collections.Counter()\n",
        "  for order in range(1, max_order + 1):\n",
        "    for i in range(0, len(segment) - order + 1):\n",
        "      ngram = tuple(segment[i:i+order])\n",
        "      ngram_counts[ngram] += 1\n",
        "  return ngram_counts\n",
        "\n",
        "def compute_bleu(reference_corpus, translation_corpus, max_order=4,\n",
        "                 smooth=False):\n",
        "  \"\"\"Computes BLEU score of translated segments against one or more references.\n",
        "\n",
        "  Args:\n",
        "    reference_corpus: list of lists of references for each translation. Each\n",
        "        reference should be tokenized into a list of tokens.\n",
        "    translation_corpus: list of translations to score. Each translation\n",
        "        should be tokenized into a list of tokens.\n",
        "    max_order: Maximum n-gram order to use when computing BLEU score.\n",
        "    smooth: Whether or not to apply Lin et al. 2004 smoothing.\n",
        "\n",
        "  Returns:\n",
        "    3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n",
        "    precisions and brevity penalty.\n",
        "  \"\"\"\n",
        "  matches_by_order = [0] * max_order\n",
        "  possible_matches_by_order = [0] * max_order\n",
        "  reference_length = 0\n",
        "  translation_length = 0\n",
        "  for (references, translation) in zip(reference_corpus,\n",
        "                                       translation_corpus):\n",
        "    reference_length += min(len(r) for r in references)\n",
        "    translation_length += len(translation)\n",
        "\n",
        "    merged_ref_ngram_counts = collections.Counter()\n",
        "    for reference in references:\n",
        "      merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n",
        "    translation_ngram_counts = _get_ngrams(translation, max_order)\n",
        "    overlap = translation_ngram_counts & merged_ref_ngram_counts\n",
        "    for ngram in overlap:\n",
        "      matches_by_order[len(ngram)-1] += overlap[ngram]\n",
        "    for order in range(1, max_order+1):\n",
        "      possible_matches = len(translation) - order + 1\n",
        "      if possible_matches > 0:\n",
        "        possible_matches_by_order[order-1] += possible_matches\n",
        "\n",
        "  precisions = [0] * max_order\n",
        "  for i in range(0, max_order):\n",
        "    if smooth:\n",
        "      precisions[i] = ((matches_by_order[i] + 1.) /\n",
        "                       (possible_matches_by_order[i] + 1.))\n",
        "    else:\n",
        "      if possible_matches_by_order[i] > 0:\n",
        "        precisions[i] = (float(matches_by_order[i]) /\n",
        "                         possible_matches_by_order[i])\n",
        "      else:\n",
        "        precisions[i] = 0.0\n",
        "\n",
        "  if min(precisions) > 0:\n",
        "    p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n",
        "    geo_mean = math.exp(p_log_sum)\n",
        "  else:\n",
        "    geo_mean = 0\n",
        "\n",
        "  ratio = float(translation_length) / reference_length\n",
        "\n",
        "  if ratio > 1.0:\n",
        "    bp = 1.\n",
        "  else:\n",
        "    bp = math.exp(1 - 1. / ratio)\n",
        "\n",
        "  bleu = geo_mean * bp\n",
        "\n",
        "  return (bleu, precisions, bp, ratio, translation_length, reference_length)\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import re\n",
        "from functools import lru_cache\n",
        "\n",
        "\n",
        "class BaseTokenizer:\n",
        "    \"\"\"A base dummy tokenizer to derive from.\"\"\"\n",
        "\n",
        "    def signature(self):\n",
        "        \"\"\"\n",
        "        Returns a signature for the tokenizer.\n",
        "        :return: signature string\n",
        "        \"\"\"\n",
        "        return \"none\"\n",
        "\n",
        "    def __call__(self, line):\n",
        "        \"\"\"\n",
        "        Tokenizes an input line with the tokenizer.\n",
        "        :param line: a segment to tokenize\n",
        "        :return: the tokenized line\n",
        "        \"\"\"\n",
        "        return line\n",
        "\n",
        "\n",
        "class TokenizerRegexp(BaseTokenizer):\n",
        "    def signature(self):\n",
        "        return \"re\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self._re = [\n",
        "            # language-dependent part (assuming Western languages)\n",
        "            (re.compile(r\"([\\{-\\~\\[-\\` -\\&\\(-\\+\\:-\\@\\/])\"), r\" \\1 \"),\n",
        "            # tokenize period and comma unless preceded by a digit\n",
        "            (re.compile(r\"([^0-9])([\\.,])\"), r\"\\1 \\2 \"),\n",
        "            # tokenize period and comma unless followed by a digit\n",
        "            (re.compile(r\"([\\.,])([^0-9])\"), r\" \\1 \\2\"),\n",
        "            # tokenize dash when preceded by a digit\n",
        "            (re.compile(r\"([0-9])(-)\"), r\"\\1 \\2 \"),\n",
        "            # one space only between words\n",
        "            # NOTE: Doing this in Python (below) is faster\n",
        "            # (re.compile(r'\\s+'), r' '),\n",
        "        ]\n",
        "\n",
        "    @lru_cache(maxsize=2**16)\n",
        "    def __call__(self, line):\n",
        "        \"\"\"Common post-processing tokenizer for `13a` and `zh` tokenizers.\n",
        "        :param line: a segment to tokenize\n",
        "        :return: the tokenized line\n",
        "        \"\"\"\n",
        "        for (_re, repl) in self._re:\n",
        "            line = _re.sub(repl, line)\n",
        "\n",
        "        # no leading or trailing spaces, single space within words\n",
        "        # return ' '.join(line.split())\n",
        "        # This line is changed with regards to the original tokenizer (seen above) to return individual words\n",
        "        return line.split()\n",
        "\n",
        "\n",
        "class Tokenizer13a(BaseTokenizer):\n",
        "    def signature(self):\n",
        "        return \"13a\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self._post_tokenizer = TokenizerRegexp()\n",
        "\n",
        "    @lru_cache(maxsize=2**16)\n",
        "    def __call__(self, line):\n",
        "        \"\"\"Tokenizes an input line using a relatively minimal tokenization\n",
        "        that is however equivalent to mteval-v13a, used by WMT.\n",
        "        :param line: a segment to tokenize\n",
        "        :return: the tokenized line\n",
        "        \"\"\"\n",
        "\n",
        "        # language-independent part:\n",
        "        line = line.replace(\"<skipped>\", \"\")\n",
        "        line = line.replace(\"-\\n\", \"\")\n",
        "        line = line.replace(\"\\n\", \" \")\n",
        "\n",
        "        if \"&\" in line:\n",
        "            line = line.replace(\"&quot;\", '\"')\n",
        "            line = line.replace(\"&amp;\", \"&\")\n",
        "            line = line.replace(\"&lt;\", \"<\")\n",
        "            line = line.replace(\"&gt;\", \">\")\n",
        "\n",
        "        return self._post_tokenizer(f\" {line} \")\n",
        "\n",
        "def calculate_blue(predictions, references, tokenizer):\n",
        "    references = [[ref] for ref in references]\n",
        "\n",
        "\n",
        "    references = [[tokenizer(r) for r in ref] for ref in references]\n",
        "    predictions = [tokenizer(p) for p in predictions]\n",
        "    score = compute_bleu(\n",
        "        reference_corpus=references, translation_corpus=predictions, max_order=4, smooth=False\n",
        "    )\n",
        "    (bleu, precisions, bp, ratio, translation_length, reference_length) = score\n",
        "\n",
        "    return {\n",
        "        \"bleu\": bleu,\n",
        "        \"precisions\": precisions,\n",
        "        \"brevity_penalty\": bp,\n",
        "        \"length_ratio\": ratio,\n",
        "        \"translation_length\": translation_length,\n",
        "        \"reference_length\": reference_length,\n",
        "    }"
      ],
      "metadata": {
        "id": "9DFWk0fWCF6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def bulk_bleuscore(premise_array,hypothesis_array,step=10000, save_name = 'all_BleuScore_scores_snli_.pkl'):\n",
        "\n",
        "    scores = {\n",
        "        'score':[],\n",
        "    }\n",
        "    for i in tqdm(range(0,len(premise_array),step)):\n",
        "        premise = premise_array[i:i+step]\n",
        "        hypothesis = hypothesis_array[i:i+step]\n",
        "\n",
        "        bleu_scores = []\n",
        "        for i in (range(len(premise))):\n",
        "            bleu_score = calculate_blue([hypothesis[i]],[premise[i]],Tokenizer13a())['bleu']\n",
        "            bleu_scores.append(bleu_score)\n",
        "\n",
        "        scores['score'] = scores['score'] + bleu_scores\n",
        "\n",
        "        del bleu_scores\n",
        "\n",
        "        with open(save_name, 'wb') as f:\n",
        "            pickle.dump(scores, f)\n",
        "\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "GdTlkLKBCYNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bulk_bleu_scores = bulk_bleuscore(s1,s2,step=10000, save_name = 'all_BleuScore_scores_snli_.pkl')"
      ],
      "metadata": {
        "id": "2URiXIjECaWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from rouge_score import rouge_scorer\n",
        "rouge_scorer1 = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "def bulk_rougescore(premise_array,hypothesis_array,step=10000, save_name = 'all_RougeScore_scores_snli_.pkl'):\n",
        "\n",
        "    scores = {\n",
        "        'rouge1':[],\n",
        "        'rouge2':[],\n",
        "        'rougeL':[],\n",
        "    }\n",
        "    for i in tqdm(range(0,len(premise_array),step)):\n",
        "        premise = premise_array[i:i+step]\n",
        "        hypothesis = hypothesis_array[i:i+step]\n",
        "\n",
        "        rougeL_scores = []\n",
        "        rouge1_scores = []\n",
        "        rouge2_scores = []\n",
        "        for i in (range(len(premise))):\n",
        "            rouge_score = rouge_scorer1.score(hypothesis[i],premise[i])\n",
        "            rouge1_scores.append(rouge_score['rouge1'].fmeasure)\n",
        "            rouge2_scores.append(rouge_score['rouge2'].fmeasure)\n",
        "            rougeL_scores.append(rouge_score['rougeL'].fmeasure)\n",
        "\n",
        "        scores['rouge1'] = scores['rouge1'] + rouge1_scores\n",
        "        scores['rouge2'] = scores['rouge2'] + rouge2_scores\n",
        "        scores['rougeL'] = scores['rougeL'] + rougeL_scores\n",
        "\n",
        "        del rougeL_scores\n",
        "        del rouge1_scores\n",
        "        del rouge2_scores\n",
        "\n",
        "        with open(save_name, 'wb') as f:\n",
        "            pickle.dump(scores, f)\n",
        "\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "DlCdIficCkAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bulk_rouge_scores = bulk_rougescore(s1,s2,step=10000, save_name = 'all_RougeScore_scores_snli_.pkl')"
      ],
      "metadata": {
        "id": "N-KGcSHACyrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/neulab/BARTScore.git"
      ],
      "metadata": {
        "id": "n4nUyKXXQXTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from os.path import dirname\n",
        "sys.path.append('./BARTScore')"
      ],
      "metadata": {
        "id": "Cc_NgHueQZ1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bart_score import BARTScorer"
      ],
      "metadata": {
        "id": "hKCSBxxRQb_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bart_scorer = BARTScorer(device='cuda:0', checkpoint='facebook/bart-large-cnn')"
      ],
      "metadata": {
        "id": "mqDCd7-IQeeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import pickle\n",
        "def bulk_bartscore(premise_array,hypothesis_array,step=10000, save_name = 'all_BARTScore_scores_snli_.pkl'):\n",
        "\n",
        "    scores = {\n",
        "        'score':[],\n",
        "    }\n",
        "    for i in tqdm(range(0,len(premise_array),step)):\n",
        "        premise = premise_array[i:i+step]\n",
        "        hypothesis = hypothesis_array[i:i+step]\n",
        "\n",
        "        score = bart_scorer.score(premise,hypothesis)\n",
        "\n",
        "        scores['score'] = scores['score'] +score\n",
        "\n",
        "        del score\n",
        "\n",
        "        with open(save_name, 'wb') as f:\n",
        "            pickle.dump(scores, f)\n",
        "\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "zc1qwTZpQiUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bulk_bart_scores = bulk_bartscore(s1,s2, step=10000, save_name = 'all_BARTScore_scores_snli_.pkl')"
      ],
      "metadata": {
        "id": "UpPsYmYNQkr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AIPHES/emnlp19-moverscore.git\n",
        "# due to deprications, you may need to use numpy 1.26.4, and you may need to go to the moverscore_v2.py file and replace all occurances of np.float with float"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neU0ttuyQqqM",
        "outputId": "b47acf3d-b09c-4358-fcbf-9589d8878b20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'emnlp19-moverscore'...\n",
            "remote: Enumerating objects: 459, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 459 (delta 21), reused 21 (delta 21), pack-reused 435 (from 2)\u001b[K\n",
            "Receiving objects: 100% (459/459), 7.07 MiB | 5.29 MiB/s, done.\n",
            "Resolving deltas: 100% (212/212), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pyemd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0d8ymQyQs28",
        "outputId": "915bab50-e17c-48e0-b585-b5a1d380fc01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyemd\n",
            "  Downloading pyemd-1.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from pyemd) (2.0.2)\n",
            "Downloading pyemd-1.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (666 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m666.6/666.6 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyemd\n",
            "Successfully installed pyemd-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pytest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u28VSRztQvDL",
        "outputId": "1c5c42d5-1c1d-4a47-a6f0-740ef7b51232"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (8.3.5)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytest) (24.2)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest) (1.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTd8iEFZSZ3C",
        "outputId": "42d6cb3e-ca2d-4bce-a67c-38e3ed0e5753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.0\n",
            "    Uninstalling numpy-1.24.0:\n",
            "      Successfully uninstalled numpy-1.24.0\n",
            "Successfully installed numpy-1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from os.path import dirname\n",
        "sys.path.append('./emnlp19-moverscore')"
      ],
      "metadata": {
        "id": "bXN_SUE4QxbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from moverscore_v2 import get_idf_dict, word_mover_score\n",
        "from collections import defaultdict\n",
        "idf_dict_hyp = defaultdict(lambda: 1.)\n",
        "idf_dict_ref = defaultdict(lambda: 1.)\n",
        "scores = word_mover_score(['This is interesting.'], ['This is fun'], idf_dict_ref, idf_dict_hyp, \\\n",
        "                          stop_words=[], n_gram=1, remove_subwords=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNSnkOo1Q2w9",
        "outputId": "f2d4b327-f73a-49f7-a43a-8016dd17912a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "DistilBertSdpaAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import pickle\n",
        "def bulk_moverscore(premise_array,hypothesis_array,step=10000, save_name = 'all_MoverScore_scores_snli_.pkl'):\n",
        "\n",
        "    scores = {\n",
        "        'score':[],\n",
        "    }\n",
        "    for i in tqdm(range(0,len(premise_array),step)):\n",
        "        premise = premise_array[i:i+step]\n",
        "        hypothesis = hypothesis_array[i:i+step]\n",
        "\n",
        "        score = word_mover_score(premise,hypothesis, idf_dict_ref, idf_dict_hyp, \\\n",
        "                          stop_words=[], n_gram=1, remove_subwords=True)\n",
        "\n",
        "        scores['score'] = scores['score'] +score\n",
        "\n",
        "        del score\n",
        "\n",
        "        with open(save_name, 'wb') as f:\n",
        "            pickle.dump(scores, f)\n",
        "\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "W0p1xaxuQ6Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bulk_mover_scores = bulk_moverscore(s1,s2, step=10000, save_name = 'all_MoverScore_scores_snli_.pkl')"
      ],
      "metadata": {
        "id": "3i_oHtVIQ8gK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "similarity_score = cosmic_scores_snli['similarity']\n",
        "consistency_score = cosmic_scores_snli['consistency']\n",
        "high_consistency = [True if x>0.5 else False for x in cosmic_scores_snli['consistency']]\n",
        "low_consistency = [True if x<=0.5 else False for x in cosmic_scores_snli['consistency']]\n",
        "\n",
        "\n",
        "correlations = []\n",
        "scores = [\n",
        "    cosmic_scores_snli['cosmic'],\n",
        "    similarity_score,\n",
        "    bulk_bert_scores['F1'],\n",
        "    bulk_bart_scores['score'],\n",
        "    bulk_mover_scores['score'],\n",
        "    bulk_bleu_scores['score'],\n",
        "    bulk_rouge_scores['rouge1'],\n",
        "    bulk_rouge_scores['rouge2'],\n",
        "    bulk_rouge_scores['rougeL'],\n",
        "]\n",
        "\n",
        "for score in scores:\n",
        "    score_consistent = list(compress(score,high_consistency))\n",
        "    similarity_consistent = list(compress(similarity_score,high_consistency))\n",
        "    score_inconsistent = list(compress(score,low_consistency))\n",
        "    similarity_inconsistent = list(compress(similarity_score,low_consistency))\n",
        "    correlations.append([np.corrcoef(similarity_score, score)[0][1], np.corrcoef(consistency_score, score)[0][1],np.corrcoef(similarity_consistent, score_consistent)[0][1], np.corrcoef(consistency_consistent, score_consistent)[0][1],np.corrcoef(similarity_inconsistent, score_inconsistent)[0][1], np.corrcoef(consistency_inconsistent, score_inconsistent)[0][1]])\n",
        "\n",
        "df_snli = pd.DataFrame(correlations,columns=['Embedding Similarity', 'Consistency','Embedding Similarity (Consistent)', 'Consistency (Consistent)','Embedding Similarity (inconsistent)', 'Consistency (inconsistent)'], index=['Cosmic','Embedding Similarity','BERTScore','BARTScore','MoverScore','BLEU', \"ROUGE-1\", 'ROUGE-2',\"ROUGE-L\"] )\n",
        "df_snli"
      ],
      "metadata": {
        "id": "e-y_zqgcRDUk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}